# Recommended CDDS Operational Procedure (v2.2.3+) for use on JASMIN

The definitions of the italicised phrases can be found in the [Glossary](https://code.metoffice.gov.uk/doc/cdds/mip_convert/html/glossary.html).

Use `<script> -h` or `<script> --help` to print information about the script, including available parameters. 

A simulation for the pre-industrial control from UKESM will be used as an example in these instructions.

## Changes relative to CDDS v2.0 on JASMIN

* The Extract, Configure, Convert, QC and Transfer steps of CDDS are now triggered by a single command line script. Command line arguments can be used to disable steps that are no required for processing (typically, Extract and Transfer)
* You can now override the data request interface with a simple file that lists all the variables that you want to process.

Note: errors of the form 
```bash
/home/h03/cdds/software/miniconda3/envs/cdds-2.0.0/lib/python3.6/site-packages/dask/config.py:161: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
```
will appear in every CDDS command for v2.2.x. These can be ignored

## CDDS Design strategy and multiple packages for a simulation

CDDS was principally designed for use at the Met Office, but most components should be usable elsewhere. 

New versions of CDDS are being released every 2-3 months on average, and introduce new variables and code improvements. New versions can alter the procedure for working with CDDS, so please ensure you use the corresponding operational procedure.

Within the Met Office multiple "packages" have been run across each simulation to incrementally produce and publish datasets for newly added and approved variables. For example, at the last count there have been over 10 packages producing and publishing data for the first of each of the UKESM1 piControl, historical and several of the climate change scenario simulations. As we move away from the CMIP6 project, new users of CDDS will probably want to run all or most of their processing in one step (i.e. as a single "package").

## Directories

There are two principal directories used by CDDS; 

**The "proc" directory under** `$ROOT_PROC_DIR`
  * Contains logs and other processing artefacts that is intended to be kept to form part of the audit trail should queries be raised over the data, e.g. `/gws/nopw/j04/mohc_shared/users/$USER/proc`

**The "data" directory under** `$ROOT_DATA_DIR`
  * Contains input model data and output netCDF files, e.g. `/gws/nopw/j04/mohc_shared/users/$USER/data`

Within the Met Office `$ROOT_PROC_DIR` and `$ROOT_DATA_DIR` are set within the CDDS code, but on JASMIN these should be provided to each CDDS command via the `-c` and `-t` arguments as indicated below. 

**Note that these values should be full rather than relative paths when supplied to CDDS Convert**, otherwise there may be problems within the rose suite.

## Requirements

1. Before running the CDDS Operational Procedure, please ensure that:
   * you have some data
   * You have updated the suite metadata as per [Rose Suite Metadata](https://code.metoffice.gov.uk/trac/ukcmip6/wiki/CMIP6/RoseSuiteMetadata) and have followed the steps in [Experiment Guidance](https://code.metoffice.gov.uk/trac/ukcmip6/wiki/ExperimentGuidance#Processanddeliver)
   * You are able to access the MOSRS via the command line. Caching your MOSRS username and password will significantly ease this process, see [here](https://code.metoffice.gov.uk/trac/home/wiki/AuthenticationCaching#JASMIN).
   * It is highly recommended that you open a processing ticket using a link from the [wiki:#CMIP6 CDDS home page]. This will allow us to provide technical support in a transparent and consistent way.

   If any of the above are not true, please contact the [CDDS Team](mailto:cdds@metoffice.gov.uk).

### Partial processing of a simulation

In certain circumstances it may be desirable to process and submit a subset of an entire simulation, i.e. the first 250 years of the `esm-piControl` simulation. Please contact Matthew Mizielinski to discuss this prior to starting processing to 

1. Get appropriate guidance on the steps needed to correctly construct the requested variables file in CDDS Prepare
2. Arrange for an appropriate [Errata](https://errata.es-doc.org/static/index.html) to be issued following submission of data sets.

## Set up the CDDS operational simulation ticket

1. **Ticket**: Select `start work` on the _CDDS operational simulation ticket_ (so that the status is `in_progress`) to indicate that work is starting. 

## Activate the CDDS install

1. Setup the environment to use the central installation of CDDS and its dependencies:
   ```bash
   source /gws/smf/j04/cmip6_prep/cdds-env-python3/miniconda3/bin/activate cdds-<cdds_version>
   ```
   where `<cdds_version>` is the version of CDDS you wish to use, e.g. `2.2.3`. Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice.
2. **Ticket**: Record the version of CDDS being used on the _CDDS operational simulation ticket_.

Notes: 
* If you wish to deactivate the CDDS environment then you can use the command `conda deactivate`.
* A full list of available cdds environments can be found using the command `conda env list`. 

## Run CDDS Prepare

### Create the request JSON file

Following the retirement of CREM the request JSON file is constructed from information in the `rose-suite.info` files within each suite. 

**IMPORTANT: if the rose-suite.info file contains incorrect information this will be propagated through CDDS. As such it is critically important that the information in these files is correct**

To construct the request JSON file take the following steps
1. Set up a working directory (this will host the request JSON file)
   ```bash
   mkdir cdds-example-1
   cd cdds-example-1
   export WORKING_DIR=`pwd`
   ```
2. Collect required information on the rose suite for the simulation;
   * suite id, e.g. `u-aw310`
   * branch, e.g. `cdds`
   * revision; If in doubt use the following command to find the latest revision of the suite branch
     ```bash
     rosie lookup --prefix=u --query project eq u-cmip6 and idx eq <suite id> and branch eq <branch>
     ```
3. Create the request JSON file;
   ```bash
   write_rose_suite_request_json <suite id> <branch> <revision> <package name> [<list of streams>]
   ```
   e.g.
   ```bash
   write_rose_suite_request_json u-aw310 cdds 115492 my_cdds_processing ap4 ap5 ap6 onm inm
   ```
   If necessary the start and end dates for processing can be overridden using the `--start_date` and `--end_date` arguments. Please consult with the [CDDS Team](mailto:cdds@metoffice.gov.uk) if you believe this is necessary.
4. For information: the log file and request JSON file are written to the current working directory

### Create the CDDS directory structure

1. Create the CDDS directory structure for the simulation; the data are stored in `<root_data_dir>`, while the processing artefacts (e.g. logs) are stored separately in `<root_proc_dir>` in directories specific to each CDDS component:
   ```bash
   create_cdds_directory_structure request.json -c $ROOT_PROC -t $ROOT_DATA
   ```
2. For information: the log file is written to the current working directory
3. Set some useful environmental variables to access the directories:
   ```bash
   export CDDS_PROC_DIR=$ROOT_PROC_DIR/<mip_era>/<mip>/<model_id>_<experiment_id>_<variant_label>/<package>/
   export CDDS_DATA_DIR=$ROOT_DATA_DIR/<mip_era>/<mip>/<model_id>/<experiment_id>/<variant_label>/<package>/
   ls $CDDS_PROC_DIR
   ls $CDDS_DATA_DIR
   ```
   Example:
   ```bash
   export CDDS_PROC_DIR=/gws/nopw/j04/mohc_shared/users/$USER/proc/CMIP6/CMIP/UKESM1-0-LL_piControl_r1i1p1f2/cdds-example-1
   export CDDS_DATA_DIR=/gws/nopw/j04/mohc_shared/users/$USER/data/CMIP6/CMIP/UKESM1-0-LL/piControl/r1i1p1f2/cdds-example-1
   ```
   Symbolic links can be used to persist these paths across sessions:
   ```bash
   ln -s $CDDS_PROC_DIR proc
   ln -s $CDDS_DATA_DIR data
   ```
   Note that these variables and links are not used by CDDS -- they are purely there for your convenience

### Generate the _requested variables list_

Generate the _requested variables list_ for the simulation:
```bash
prepare_generate_variable_list request.json -p  -c $ROOT_PROC -t $ROOT_DATA
```
This command would automatically determine which variables need to be produced for a list of streams contained in the `request.json` file. For many CMIP6 simulations this would result with no or just a few new variables, because all others have already been processed and published.

If you wish to override this behaviour, the `--no_inventory_check` flag can be used. This will turn off disabling processing of variables that have been already published.

With version `2.2.3` of CDDS, you can also override the data request entirely, relying instead on a list of variables that need to get processed. The list needs to be provided to the script as the `-r` (`--user_request_variables`) argument, in a text file with the following syntax `<mip_table>/<variable_name>:<stream>(/<substream>)`, with `<stream>(/<substream>)` being optional if you want to use standard CMIP6 variable setup. 

For instance, your `variables_to_process.txt` file could contain the following four lines
```bash
Amon/tas
Omon/tos
Amon/pr:apm
Omon/epcalc100:onm/ptrd-T
```

`Amon/tas` and `Omon/tos` use CMIP6 defaults (`ap5` and `onm/grid-T`). `Ä„mon/pr` specifies the `apm` stream explicitly, similarly to process `Omon/epcalc100` CDDS is instructed to pull data from the `onm/ptrd-T` (sub)stream.

To summarise, with this custom workflow, you would like to use the following command

```bash
prepare_generate_variable_list request.json -p  -c $ROOT_PROC -t $ROOT_DATA -r variables_to_process.txt --no_inventory_check
```

1. Run the following command; add any output to a [new ticket](https://code.metoffice.gov.uk/trac/cdds/newticket?milestone=Backlog&component=CDDS+Prepare&summary=Critical+issues+from+CDDS+Prepare+for+CMIP6+simulation+<model_id>+<experiment_id>+<variant_id>):
   ```bash
   grep CRITICAL $CDDS_PROC_DIR/prepare/log/prepare_generate_variable_list_<datetime>.log | cut -f 5- -d " "
   ```
   where `<datetime>` is of the form `YYYY-MM-DDTHHMMZ` then add a comment to the **CDDS operational simulation ticket** specifying the ticket number of the new ticket (`#<ticket_number>`)
2. For information: the _requested variables list_ is written to `$CDDS_PROC_DIR/prepare/`
3. Deactivate requested variables with _known issues_, as described in the page[Prepare Deactivate Variables](Prepare-Deactivate-Variables), additionally the appropriate page should be linked to from the _CDDS operational simulation ticket_.
   **UPDATED: from v2.0.6 most of the deactivation commands are applied automatically. There few cases where manual intervention is needed; emissions driven and certain AerChemMIP and CFMIP experiments**
4. **Ticket**: Add a comment to the _CDDS operational simulation ticket_ specifying that CDDS Prepare has been completed

### Understanding the _requested variables list_

The requested variables list is a JSON format file containing some information on the experiment being processed obtained from the CMIP6 data request. To allow users to explore the variable list there is now a tool to explore this file;
   ```bash
   create_variables_table_file $RVF table.txt
   ```
where `$RVF` contains the location of the requested variables file in the `$CDDS_PROC_DIR/prepare`. This file is a tab separated file (by default), with columns describing various properties of each variable.

### Extending the _requested variables list_

The CMIP6 data request is used to construct the list of variables to be included in the processing. Should you wish to extend this list then the `prepare_alter_variable_list` command can be used;
   ```bash
   prepare_alter_variable_list $RVF insert mip_table/var_name mip_table/var_name .... "Comment on why variable is being inserted"
   ```

Please ensure that any variables that are inserted are noted on the ticket.

Alternatively, you can just use the `--user_request_variables` argument with a list of all variables that you want to process.

## Run CDDS Extract

### Option 1: Set up input data for processing

Where data is already on disk, i.e. it came from Archer or similar platforms, CDDS Extract is not required, but removing halo rows and columns from NEMO/MEDUSA data is (see below). Note that data should be arranged in the structure
```bash
   $CDDS_DATA_DIR/
       input/
           <suite id>/
               <stream 1>/
                   <files>
               <stream 2>/
                   <files>
```


**Removing the Halo from ocean data**

The ocean output from NEMO and MEDUSA contains halo rows and columns with repeated data (this allows easier gradient calculations within the model). We strip these off to avoid presenting data with duplicate data that could confuse inexperienced users.

The `remove_ocean_haloes` script is designed to take a target directory location and a list of files to process, i.e.
   ```bash
   remove_ocean_haloes <target directory> [<file1> <file2> <file3> ...]
   ```
e.g.
   ```bash
   remove_ocean_haloes ready_to_process raw/nemo_be509o_1m_20150101-20150201_grid-T.nc raw/medusa_be509o_1m_20150101-20150201_ptrc-T.nc
   # or more simply
   remove_ocean_haloes ready_to_process raw/*.nc
   ```

Note that the files must have the correct nemo/medusa file name format and CRITICAL errors will be written to a log file. If a command fails to complete (e.g. session on JASMIN is killed) re-running the command should recover from where it left off. If in doubt contact [CDDS Team](mailto:cdds@metoffice.gov.uk).

Removing haloes is not a fast process; on LOTUS it runs at around 1.6 GB per hour for a single file at a time. On [LotusParallelHaloRemoval](https://code.metoffice.gov.uk/trac/cdds/wiki/CDDSExtract/LotusParallelHaloRemoval) you will find the template for launching a [job array](https://help.jasmin.ac.uk/article/4890-how-to-submit-a-job-to-slurm).


**Constructing the correct directory structure**

If you do not have the directory configuration noted above, but you have data organised by year (as generated on ARCHER) the input folder can be rearranged by grouping data files by stream, using the `path_reformatter` script. 

   ```bash
   path_reformatter --input_dir $CDDS_DATA_DIR/input --suite_id <suite id> --streams <stream names>
   ```
Example:
   ```bash
   path_reformatter --input_dir $CDDS_DATA_DIR/input --suite_id u-bq244 --streams ap4 ap5 inm onm
   ```

This command will create `ap4`, `ap5`, `inm`, and `onm` directories in `$CDDS_DATA_DIR/input/u-bq244`, and populate them with symlinks to all relevant input files.

### Option 2: Run CDDS Extract

This option is available for simulations run at the Met Office and archived to MASS. This uses filtered extraction and as noted above this can have a significant impact on the performance of CDDS (less data => faster). Please note that data extraction is now also a part of the `cdds_convert` command, so if you want to run it as a separate step it will need to be skipped later, in the convert stage (using the `--skip_extract` command line argument). It is highly recommended that users who wish to use the simpler workflow (i.e. generate `request->create directory structure->run prepare->run convert`) would use the following instructions to test the performance of MASS interface first.

1. ssh to `mass-cli1` and confirm that you can access data in MASS.
2. Extract the _model output files_ for the simulation from MASS (note: this will take up to two days to complete):
   ```bash
   cdds_extract_spice request.json -c $ROOT_PROC -t $ROOT_DATA
   ```
3. Monitor the progress of the job. Progress information is written to the extract log file (`$CDDS_PROC_DIR/extract/log/cdds_extract_<datetime>.log`)
4. Once the job has completed run the following command to check for CRITICAL issues:
   ```bash
   grep CRITICAL $CDDS_PROC_DIR/extract/log/cdds_extract_<datetime>.log
   ```
   If there is any output:
      * if the log file output contains only MOOSE errors (see [CDDSMooseFailures](https://code.metoffice.gov.uk/trac/cdds/wiki/Extract/CDDSMooseFailures) for examples), add the error to [CDDSMooseFailures](https://code.metoffice.gov.uk/trac/cdds/wiki/Extract/CDDSMooseFailures), then rerun Extract
     * if the log file states the extraction has successfully completed, look for `exit_nicely` at the end of the log file, add any output to a [new ticket](https://code.metoffice.gov.uk/trac/cdds/newticket?milestone=Backlog&component=Extract&summary=Critical+issues+from+Extract+for+CMIP6+simulation+<model_id>+<experiment_id>+<variant_id>), then add a comment to the _CDDS operational simulation ticket_ specifying the ticket number of the new ticket (`#<ticket_number>`)
5. If a `CRITICAL` issue is reported it is important that you check for a new error;
   ```bash
   grep -i "HDF error" $CDDS_PROC_DIR/extract/log/cdds_extract_<datetime>.log
   ```
   The occurrence of this error is connected with corrupted files being extracted from netCDF streams. If this error is seen please contact 
[CDDS Team](mailto:cdds@metoffice.gov.uk) with "HDF error in CDDS Extract" as the subject to discuss remedial action before proceeding.
6. If the validation log files (`$CDDS_PROC_DIR/extract/log/*_validation.txt`) has been created, add any output to a [new ticket](https://code.metoffice.gov.uk/trac/cdds/newticket?milestone=Backlog&component=Extract&summary=Critical+issues+from+Extract+for+CMIP6+simulation+<model_id>+<experiment_id>+<variant_id>)
7. For information: the _model output files_ are written to `$CDDS_DATA_DIR/input/`
8. **Ticket**: Add a comment to the ''CDDS operational simulation ticket'' specifying that CDDS Extract has been completed

## Run CDDS Convert

1. Connect to the cylc server `cylc1` on JASMIN and export variables used by the JASMIN installation of CDDS (under investigation);
   ```bash
   ssh cylc1
   export TMPDIR=$PWD  # This should not be used but may be causing issues if unset.
   ```
2. Produce the _output netCDF files_ for the simulation:
   ```bash
   cdds_convert request.json -c $ROOT_PROC -t $ROOT_DATA --skip_extract --skip_transfer
   ```
   This will launch a series of Rose suite, which would configure and then trigger processing, one for each stream for which there is processing to do (if a stream is included in the request JSON file, but there are no variables that can be produced from that stream it will be skipped), and then it will run QC checks on resulting datasets.

If you want to fetch data from MASS and didn't use the `Run CDDS Extract` option above, you can run the `cdds_convert` command without the `--skip_extract` flag. Prerequisites for processing are similar, i.e. you need to ssh into `mass-cli1` and have access to MASS archive. 

1. Monitor the progress of the suite using:
   * the suite control GUI (use `rose sgc --name=cdds_<model_id>_<experiment_id>_<variant_label>_<stream_id>`, e.g. `rose sgc --name=cdds_UKESM1-0-LL_piControl_r1i1p1f2_onm`)
   * the cylc scan GUI for monitoring running suites (use `cylc gscan -n cdds*`)
   * [cylc review](http://fcm1/cylc-review/) via `http://fcm1/cylc-review/suites?user=<username>`
2. Each suite will send an e-mail when it completes or stalls (i.e. tasks fail in such a way the suite cannot continue). If a suite stalls investigate the logging information available in the job.err files for failed tasks within the suite in order to diagnose issues and contact cdds@metoffice.gov.uk for assistance.
3. Wait for **ALL** suites to complete. You can check the status of all suites using the `cylc gscan` command or cylc review (see above for links). Note that it can take several days for these suites to complete depending on the volume of data to be produced. Once CDDS the suites from all streams in a package have completed, continue with the next action.
4. If the critical log file (`$CDDS_PROC_DIR/convert/log/critical_issues.log`) has been created run the following command; add any output to a [new ticket](https://code.metoffice.gov.uk/trac/cdds/newticket?milestone=Backlog&component=MIP+Convert&summary=Critical+issues+from+MIP+Convert+for+CMIP6+simulation+<model_id>+<experiment_id>+<variant_id>):
   ```bash
   cut -f 5- -d " " $CDDS_PROC_DIR/convert/log/critical_issues.log | sort -u
   ```
   then add a comment to the _CDDS operational simulation ticket_ specifying the ticket number of the new ticket (`#<ticket_number>`)
5. Run the following command; add any output to a [new ticket](https://code.metoffice.gov.uk/trac/cdds/newticket?milestone=Backlog&component=CDDS+Convert&summary=Critical+issues+from+CDDS+Convert+for+CMIP6+simulation+<model_id>+<experiment_id>+<variant_id>):
   ```bash
   grep CRITICAL $CDDS_PROC_DIR/convert/log/mip_concatenate_*.log
   ```
   then add a comment to the _CDDS operational simulation ticket_ specifying the ticket number of the new ticket (`#<ticket_number>`)
6. For information: the _output netCDF files_ are written to `$CDDS_DATA_DIR/output/`.
7. **Ticket**: Add a comment to the _CDDS operational simulation ticket_ specifying that CDDS Convert has been completed.

## Check results of CDDS QC

The CDDS QC tool double checks that all files match the CMIP6 requirements and looks for missing time records. The purpose of CDDS QC is to identify problems with data sets before they are published, as the effort required to report issues via the [errata system](https://errata.es-doc.org), retract and replace them can be significant.

1. For information: the log files are written to `$CDDS_PROC_DIR/qualitycheck/log/`
2. For information: the report is written to `$CDDS_PROC_DIR/qualitycheck/report_<datetime>.json`. 
3. If the report written to `$CDDS_PROC_DIR/qualitycheck/report_<datetime>.json` is **not** empty (i.e. if the `aggregated_summary` and `details` sections of this file are **not** empty, see [here](https://code.metoffice.gov.uk/trac/cdds/wiki/Quality/ExampleJSONReport) for an example) add any output from the report to a [new ticket](https://code.metoffice.gov.uk/trac/cdds/newticket?milestone=Backlog&component=Quality+Control&summary=Critical+issues+from+CDDS+QC+for+CMIP6+simulation+<model_id>+<experiment_id>+<variant_id>).
4. For information: CDDS QC also writes an approved variables file `approved_variables_<datetime>.json` that lists all the variables that passed QC, and the location in which the files are found. This is used by CDDS Transfer and other tools.
5. **Ticket**: Add a comment to the _CDDS operational simulation ticket_ specifying that CDDS QC has been completed

### Retaining information from the CDDS Proc directory

The CDDS proc directory contains all the processing logs and artefacts that we would use to interpret how processing has progressed. Should issues be found with the data later there are various files that we can look into to understand whether there were issues in the processing that were missed. We recommend that you retain the contents of the proc directories, perhaps in the form of a tarball (e.g. `tar -czf cdds_proc_<model_id>_<experiment_id>_<variant_label>_<package_name>.tgz $CDDS_PROC_DIR`).

### Close ticket
**Ticket**: Update and close the _CDDS operational simulation ticket_