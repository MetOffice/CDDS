{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Climate Data Dissemination System The Climate Data Dissemination System (CDDS) is a Python-based system that manages the reprocessing of HadGEM3 and UKESM1 climate model data into a standards compliant ( CMOR ) form suitable for publication and sharing. The primary driver behind CDDS was the CMIP6 project and CDDS was used, and is continuing to be used, to deliver a large amount of data to the Centre for Environmental Data Archival (CEDA) for publication to ESGF. CDDS has recently been adapted to allow for the easy addition of both models and projects, provided that they follow the structure for CMIP6, i.e. via predefined activities (MIPS), source ids (models) and experiments. As of version 2.2.4 CDDS supports production of data for CMIP6 and for a Met Office internal project GCModelDev where Met Office scientists are encouraged to request additional activities and experiments that can be used to support their science. Note that the GCModelDev project is not intended to prepare data for publication -- anyone wanting to publish data or prepare it for an external project is encouraged to contact the CDDS team or start a discussion here. Useful Documents: Operational Procedures Contact You can contact the CDDS team via cdds@metoffice.gov.uk . Contributing to CDDS The development work for CDDS is managed via JIRA Diagnostic Review Science QA review of mappings for CMIP6 CMIP6 Processing Simulation tickets for CMIP6 work can be raised on the CDDS Trac system (SRS login required) via Open a new operational simulation ticket (V2.2) Additional documentation Extract Quality Control Links to be migrated * [[Status of processing for CMIP6 simulations|https://code.metoffice.gov.uk/trac/cdds/wiki/CMIP6Simulations]] * [[Status of IPCC MIP requested variables|https://code.metoffice.gov.uk/trac/cdds/wiki/CMIP6IPCCVariables]] * [[Simulation Ticket Review Procedure|Simulation-Ticket-Review-Procedure]] * [[Analysis of variables requiring work as of CDDS v2.0.4|https://code.metoffice.gov.uk/trac/cdds/wiki/VariableReportCDDS_v121]]","title":"About"},{"location":"#climate-data-dissemination-system","text":"The Climate Data Dissemination System (CDDS) is a Python-based system that manages the reprocessing of HadGEM3 and UKESM1 climate model data into a standards compliant ( CMOR ) form suitable for publication and sharing. The primary driver behind CDDS was the CMIP6 project and CDDS was used, and is continuing to be used, to deliver a large amount of data to the Centre for Environmental Data Archival (CEDA) for publication to ESGF. CDDS has recently been adapted to allow for the easy addition of both models and projects, provided that they follow the structure for CMIP6, i.e. via predefined activities (MIPS), source ids (models) and experiments. As of version 2.2.4 CDDS supports production of data for CMIP6 and for a Met Office internal project GCModelDev where Met Office scientists are encouraged to request additional activities and experiments that can be used to support their science. Note that the GCModelDev project is not intended to prepare data for publication -- anyone wanting to publish data or prepare it for an external project is encouraged to contact the CDDS team or start a discussion here.","title":"Climate Data Dissemination System"},{"location":"#useful-documents","text":"Operational Procedures","title":"Useful Documents:"},{"location":"#contact","text":"You can contact the CDDS team via cdds@metoffice.gov.uk .","title":"Contact"},{"location":"#contributing-to-cdds","text":"The development work for CDDS is managed via JIRA","title":"Contributing to CDDS"},{"location":"#diagnostic-review","text":"Science QA review of mappings for CMIP6","title":"Diagnostic Review"},{"location":"#cmip6-processing","text":"Simulation tickets for CMIP6 work can be raised on the CDDS Trac system (SRS login required) via Open a new operational simulation ticket (V2.2)","title":"CMIP6 Processing"},{"location":"#additional-documentation","text":"Extract Quality Control Links to be migrated * [[Status of processing for CMIP6 simulations|https://code.metoffice.gov.uk/trac/cdds/wiki/CMIP6Simulations]] * [[Status of IPCC MIP requested variables|https://code.metoffice.gov.uk/trac/cdds/wiki/CMIP6IPCCVariables]] * [[Simulation Ticket Review Procedure|Simulation-Ticket-Review-Procedure]] * [[Analysis of variables requiring work as of CDDS v2.0.4|https://code.metoffice.gov.uk/trac/cdds/wiki/VariableReportCDDS_v121]]","title":"Additional documentation"},{"location":"cdds_components/convert/","text":"Warning This documentation is currently under construction and may not be up to date.","title":"convert"},{"location":"cdds_components/extract/","text":"Warning This documentation is currently under construction and may not be up to date. CDDS Extract The Extract package enables a user to extract a subset of climate model output files from the MASS tape archive. This is useful because raw atmosphere and ocean output is stored in massive files, containing multiple fields, and their large volume make all processing very slow. Before data transfer happens, Extract selects only relevant bits of output, necessary to produce the requested, CMORised variables. Since CDDS 2.2, Extract is embedded within the cdds_convert script, and it is no longer necessary to run it as a separate process. It is also worth mentioning that despite working with only a subset of climate model output, data transfer from MASS is typically the longest step in the whole CDDS processing pipeline, can take many days, and is prone to failures caused by MASS misbehaviour. An overview of Extract The user configures the extraction process using the request file. The cdds_extract script then performs the following processing steps: retrieves the data dissemination configuration from the request file. retrieves the requested variables list for the experiment. retrieves the model to MIP mapping for each MIP requested variable logs information on the variables for which authorised mappings are available and variables/mappings that are not available and why. creates directories for holding the retrieved data from MASS, the post-processed data which will be disseminated and the directories used to hold the artifacts used by other cdds processes (e.g. log files) creates appropriate PP and netCDF variable filter files that are used to filter the data retrieved from MASS before it is copied to disk. checks that MASS holds the relevant data collections and constructs MOOSE commands that will retrieve the required model data from these data collections (optionally incorporating the relevant filter files) submits constructed MOOSE commands and reports progress in text logs performs validation on the retrieved data to check it meets processing requirement","title":"extract"},{"location":"cdds_components/extract/#cdds-extract","text":"The Extract package enables a user to extract a subset of climate model output files from the MASS tape archive. This is useful because raw atmosphere and ocean output is stored in massive files, containing multiple fields, and their large volume make all processing very slow. Before data transfer happens, Extract selects only relevant bits of output, necessary to produce the requested, CMORised variables. Since CDDS 2.2, Extract is embedded within the cdds_convert script, and it is no longer necessary to run it as a separate process. It is also worth mentioning that despite working with only a subset of climate model output, data transfer from MASS is typically the longest step in the whole CDDS processing pipeline, can take many days, and is prone to failures caused by MASS misbehaviour.","title":"CDDS Extract"},{"location":"cdds_components/extract/#an-overview-of-extract","text":"The user configures the extraction process using the request file. The cdds_extract script then performs the following processing steps: retrieves the data dissemination configuration from the request file. retrieves the requested variables list for the experiment. retrieves the model to MIP mapping for each MIP requested variable logs information on the variables for which authorised mappings are available and variables/mappings that are not available and why. creates directories for holding the retrieved data from MASS, the post-processed data which will be disseminated and the directories used to hold the artifacts used by other cdds processes (e.g. log files) creates appropriate PP and netCDF variable filter files that are used to filter the data retrieved from MASS before it is copied to disk. checks that MASS holds the relevant data collections and constructs MOOSE commands that will retrieve the required model data from these data collections (optionally incorporating the relevant filter files) submits constructed MOOSE commands and reports progress in text logs performs validation on the retrieved data to check it meets processing requirement","title":"An overview of Extract"},{"location":"cdds_components/qc/","text":"Warning This documentation is currently under construction and may not be up to date.","title":"qc"},{"location":"developer_documentation/building_documentation/","text":"Warning This documentation is currently under construction and may not be up to date. Modifying and Building the CDDS Documentation This page gives an overview of the general philosophy of the CDDS documentation. The current documentation is based upon mkdocs and is built and managed using two main packages. mkdocs-material - A particular mkdocs theme which also extends the base functionality. mike - A tool for managing multiple independent versions of documentation on a particular branch. For most purposes it is only neccessary to reference the mkdocs-material package documentation and the mike documentation. Working with mkdocs The source files are written in markdown, and are kept in the docs directory of the repository. Configuration of the building of mkdocs is done using the mkdocs.yml file. Whilst editing or adding documentation you can preview changes in realtime using by starting a local server. mkdocs serve To generate the actual site that can be uploaded to a web server you would use. mkdocs build However, it should not be needed to run this command directly in order to build and deploy the docs. This is done using the mike package (see next section). Working with mike It is worth familiarising with the overview of mike here. In short though, mike makes it relatively straightforward to manage multiple versions of docs by running the mkdocs build command and automatically commiting this to the gh-pages branch. mike deploy 3.0 Similarly to mkdocs serve , you can use the following command to start a local server to preview the docs. mike serve However, rather than displaying the docs in your CWD , this will serve the docs from the gh-pages branch. Managing Versions and Deployment in Practice Deploying the documentation in practice","title":"Documentation"},{"location":"developer_documentation/building_documentation/#modifying-and-building-the-cdds-documentation","text":"This page gives an overview of the general philosophy of the CDDS documentation. The current documentation is based upon mkdocs and is built and managed using two main packages. mkdocs-material - A particular mkdocs theme which also extends the base functionality. mike - A tool for managing multiple independent versions of documentation on a particular branch. For most purposes it is only neccessary to reference the mkdocs-material package documentation and the mike documentation.","title":"Modifying and Building the CDDS Documentation"},{"location":"developer_documentation/building_documentation/#working-with-mkdocs","text":"The source files are written in markdown, and are kept in the docs directory of the repository. Configuration of the building of mkdocs is done using the mkdocs.yml file. Whilst editing or adding documentation you can preview changes in realtime using by starting a local server. mkdocs serve To generate the actual site that can be uploaded to a web server you would use. mkdocs build However, it should not be needed to run this command directly in order to build and deploy the docs. This is done using the mike package (see next section).","title":"Working with mkdocs"},{"location":"developer_documentation/building_documentation/#working-with-mike","text":"It is worth familiarising with the overview of mike here. In short though, mike makes it relatively straightforward to manage multiple versions of docs by running the mkdocs build command and automatically commiting this to the gh-pages branch. mike deploy 3.0 Similarly to mkdocs serve , you can use the following command to start a local server to preview the docs. mike serve However, rather than displaying the docs in your CWD , this will serve the docs from the gh-pages branch.","title":"Working with mike"},{"location":"developer_documentation/building_documentation/#managing-versions-and-deployment-in-practice","text":"Deploying the documentation in practice","title":"Managing Versions and Deployment in Practice"},{"location":"developer_documentation/coding_guidelines/","text":"Warning This documentation is currently under construction. Coding Guidelines Code Style Use the \"Style Guide for Python Code\" http://www.python.org/dev/peps/pep-0008. CDDS packages should contain a package_name/package_name/tests/test_coding_standards.py module that uses the https://pypi.python.org/pypi/pep8 package to check PEP 8 conformance. In addition to this, it is recommended to use a tool that checks for errors in Python code, coding standard (e.g., PEP 8) conformance and general code quality e.g., http://www.pylint.org/. However, some http://www.python.org/dev/peps/pep-0008 guidelines are not checked by these tools; please also: Be consistent within a module or function [reference]. In Python, single-quoted strings and double-quoted strings are the same; pick a rule and stick to it [reference]. Limit all lines to a maximum of 120 characters and ?? for docstrings and comments. (Originally this was 79 as per the PEP 8 guidelines [reference]). Use Python's implied line continuation inside parentheses, brackets and braces to wrap long lines, rather than using a backslash [reference]. Add a new line before a binary operator, rather than after [reference]. Use blank lines in functions, sparingly, to indicate logical sections [reference]. Always use double quote characters for triple-quoted strings \"\"\" to be consistent with the docstring conventions in PEP 257 and PEP 8. Write comments using complete sentences [reference]. Use the https://docs.python.org/3.8/library/stdtypes.html#str.format str.format() method to perform a string formatting operation, e.g., 'Coordinates: {latitude}, {longitude}'.format(latitude='37.24N', longitude='-115.81W'), since it is the new standard in Python 3 and is preferred to the % formatting. Include the line (c) British Crown Copyright [year of creation in the form ]-[year of last modification in the form ], Met Office. as a comment at the top of every module. Imports Use absolute imports https://www.python.org/dev/peps/pep-0328/#rationale-for-absolute-imports as they are recommended by PEP 8. Also, being able to tell immediately where the function comes from greatly improves code readability and comprehension (Readability Counts). Example: o import my_package.my_subpackage.my_module and use the my_module.my_function syntax, e.g., import os; os.walk. o from my_package.my_subpackage.my_module import my_function and use my_function directly. group imports, with a blank line between each group, in the following order: standard library imports, related third party imports, local application/library specific imports [reference]. Within each import group (see above), order the imports alphabetically. place module level \"dunders\" after the module docstring but before any import statements except from future imports [reference]. Use the pattern import numpy as np. Naming Conventions Abstract Base Classes should have a name that contains Abstract for clarity. Nouns should be used when naming classes. Use descriptive names that clearly convey a meaning; refrain from using overly general / ambiguous names e.g., data. Use the .cfg extension for configuration files, e.g. those read by configparser. Typing Use https://docs.python.org/3/library/typing.html for adding type hints and annotations. Use https://mypy.readthedocs.io/en/stable/ for running static code analysis using aforementioned type hints. Docstrings Warning Historically CDDS used the NumpyDoc format for all docstrings. As of 2022 the pep-257 was adopted. This means there is a mix of docstrings formats used. Docstrings should now be written using https://docutils.sourceforge.io/rst.html as recommended by https://www.python.org/dev/peps/pep-0287/, and is rendered using Sphinx. An detailed example of the reStructuredText style docstrings can be found here https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html o Use double backticks `` around argument names so that they are rendered as code in the HTML produced by Sphinx o Use the appropriate substitutions for glossary terms. o Make use of the docstring conventions http://www.python.org/dev/peps/pep-0257. o The docstring is a phrase ending in a period and prescribes the function or method's effect as a command, not as a description [reference]. It is not necessary to write docstrings for non-public classes, methods and functions, see cdds/pylintrc and mip_convert/pylintrc. (The maintenance overhead is reduced when refactoring non-public classes, methods and functions). reStructuredText Docstring Example Below is an example docstring incorporating all of the guidelines above. def my_function ( my_param1 : float , my_param2 : str ) -> int : \"\"\" Return something. Here's a longer description about the something that is returned. It's so long, it goes over one line! :param my_param1: Description of the first parameter ``my_param1``. :type my_param1: float :param my_param2: Description of the second parameter ``my_param2``. :type my_param2: string :raises ValueError If ``my_param1`` is less than 0. :return: Description of anonymous integer return value. :rtype: int \"\"\" Doctests Where appropriate, a https://docs.python.org/3.8/library/doctest.html should be included in an Examples section of the docstring https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard. When multiple examples are provided, they should be separated by blank lines. Comments explaining the examples should have blank lines both above and below them. Scripts Script names should not have an extension, should be lowercase, and with words separated by underscores as necessary to improve readability, e.g., just_do_it. It is recommended that scripts call a main() function located in an importable module so that it is possible to run the code in the script from the Python interpreter / a test module e.g., import my_module; my_module.main(). https://docs.python.org/3.8/library/argparse.html should be used to parse command line options. def main ( args ): # Parse the arguments. args = parse_args ( args ) # Create the configured logger. configure_logger ( args . log_name , args . log_level , args . append_log ) # Retrieve the logger. logger = logging . getLogger ( __name__ ) try : exit_code = my_func ( args ) except BaseException as exc : exit_code = 1 logger . exception ( exc ) return exit_code Bash Scripts Use the Google https://google.github.io/styleguide/shellguide.html for bash scripts, as recommended by the Cylc documentation https://cylc.github.io/cylc-doc/stable/html/workflow-design-guide/general-principles.html#coding-standards Run scripts through https://www.shellcheck.net/ for catching possible bad practice. (The latest version can be installed using conda for easier access and improvements over the centrally installed version)","title":"Coding Guidelines"},{"location":"developer_documentation/coding_guidelines/#coding-guidelines","text":"","title":"Coding Guidelines"},{"location":"developer_documentation/coding_guidelines/#code-style","text":"Use the \"Style Guide for Python Code\" http://www.python.org/dev/peps/pep-0008. CDDS packages should contain a package_name/package_name/tests/test_coding_standards.py module that uses the https://pypi.python.org/pypi/pep8 package to check PEP 8 conformance. In addition to this, it is recommended to use a tool that checks for errors in Python code, coding standard (e.g., PEP 8) conformance and general code quality e.g., http://www.pylint.org/. However, some http://www.python.org/dev/peps/pep-0008 guidelines are not checked by these tools; please also: Be consistent within a module or function [reference]. In Python, single-quoted strings and double-quoted strings are the same; pick a rule and stick to it [reference]. Limit all lines to a maximum of 120 characters and ?? for docstrings and comments. (Originally this was 79 as per the PEP 8 guidelines [reference]). Use Python's implied line continuation inside parentheses, brackets and braces to wrap long lines, rather than using a backslash [reference]. Add a new line before a binary operator, rather than after [reference]. Use blank lines in functions, sparingly, to indicate logical sections [reference]. Always use double quote characters for triple-quoted strings \"\"\" to be consistent with the docstring conventions in PEP 257 and PEP 8. Write comments using complete sentences [reference]. Use the https://docs.python.org/3.8/library/stdtypes.html#str.format str.format() method to perform a string formatting operation, e.g., 'Coordinates: {latitude}, {longitude}'.format(latitude='37.24N', longitude='-115.81W'), since it is the new standard in Python 3 and is preferred to the % formatting. Include the line (c) British Crown Copyright [year of creation in the form ]-[year of last modification in the form ], Met Office. as a comment at the top of every module.","title":"Code Style"},{"location":"developer_documentation/coding_guidelines/#imports","text":"Use absolute imports https://www.python.org/dev/peps/pep-0328/#rationale-for-absolute-imports as they are recommended by PEP 8. Also, being able to tell immediately where the function comes from greatly improves code readability and comprehension (Readability Counts). Example: o import my_package.my_subpackage.my_module and use the my_module.my_function syntax, e.g., import os; os.walk. o from my_package.my_subpackage.my_module import my_function and use my_function directly. group imports, with a blank line between each group, in the following order: standard library imports, related third party imports, local application/library specific imports [reference]. Within each import group (see above), order the imports alphabetically. place module level \"dunders\" after the module docstring but before any import statements except from future imports [reference]. Use the pattern import numpy as np.","title":"Imports"},{"location":"developer_documentation/coding_guidelines/#naming-conventions","text":"Abstract Base Classes should have a name that contains Abstract for clarity. Nouns should be used when naming classes. Use descriptive names that clearly convey a meaning; refrain from using overly general / ambiguous names e.g., data. Use the .cfg extension for configuration files, e.g. those read by configparser.","title":"Naming Conventions"},{"location":"developer_documentation/coding_guidelines/#typing","text":"Use https://docs.python.org/3/library/typing.html for adding type hints and annotations. Use https://mypy.readthedocs.io/en/stable/ for running static code analysis using aforementioned type hints.","title":"Typing"},{"location":"developer_documentation/coding_guidelines/#docstrings","text":"Warning Historically CDDS used the NumpyDoc format for all docstrings. As of 2022 the pep-257 was adopted. This means there is a mix of docstrings formats used. Docstrings should now be written using https://docutils.sourceforge.io/rst.html as recommended by https://www.python.org/dev/peps/pep-0287/, and is rendered using Sphinx. An detailed example of the reStructuredText style docstrings can be found here https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html o Use double backticks `` around argument names so that they are rendered as code in the HTML produced by Sphinx o Use the appropriate substitutions for glossary terms. o Make use of the docstring conventions http://www.python.org/dev/peps/pep-0257. o The docstring is a phrase ending in a period and prescribes the function or method's effect as a command, not as a description [reference]. It is not necessary to write docstrings for non-public classes, methods and functions, see cdds/pylintrc and mip_convert/pylintrc. (The maintenance overhead is reduced when refactoring non-public classes, methods and functions). reStructuredText Docstring Example Below is an example docstring incorporating all of the guidelines above. def my_function ( my_param1 : float , my_param2 : str ) -> int : \"\"\" Return something. Here's a longer description about the something that is returned. It's so long, it goes over one line! :param my_param1: Description of the first parameter ``my_param1``. :type my_param1: float :param my_param2: Description of the second parameter ``my_param2``. :type my_param2: string :raises ValueError If ``my_param1`` is less than 0. :return: Description of anonymous integer return value. :rtype: int \"\"\"","title":"Docstrings"},{"location":"developer_documentation/coding_guidelines/#doctests","text":"Where appropriate, a https://docs.python.org/3.8/library/doctest.html should be included in an Examples section of the docstring https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard. When multiple examples are provided, they should be separated by blank lines. Comments explaining the examples should have blank lines both above and below them. Scripts Script names should not have an extension, should be lowercase, and with words separated by underscores as necessary to improve readability, e.g., just_do_it. It is recommended that scripts call a main() function located in an importable module so that it is possible to run the code in the script from the Python interpreter / a test module e.g., import my_module; my_module.main(). https://docs.python.org/3.8/library/argparse.html should be used to parse command line options. def main ( args ): # Parse the arguments. args = parse_args ( args ) # Create the configured logger. configure_logger ( args . log_name , args . log_level , args . append_log ) # Retrieve the logger. logger = logging . getLogger ( __name__ ) try : exit_code = my_func ( args ) except BaseException as exc : exit_code = 1 logger . exception ( exc ) return exit_code","title":"Doctests"},{"location":"developer_documentation/coding_guidelines/#bash-scripts","text":"Use the Google https://google.github.io/styleguide/shellguide.html for bash scripts, as recommended by the Cylc documentation https://cylc.github.io/cylc-doc/stable/html/workflow-design-guide/general-principles.html#coding-standards Run scripts through https://www.shellcheck.net/ for catching possible bad practice. (The latest version can be installed using conda for easier access and improvements over the centrally installed version)","title":"Bash Scripts"},{"location":"developer_documentation/development_practices/","text":"Warning This documentation is currently under construction and may not be up to date. Quickstart","title":"Development Practices"},{"location":"developer_documentation/development_practices/#quickstart","text":"","title":"Quickstart"},{"location":"developer_documentation/github/","text":"Warning This documentation is currently under construction and may not be up to date. Getting up to speed with git Clone working copy General Clone the CDDS project Using SSH Key Generate SSH key for Github using Seahorse: Add your SSH key to Github Clone working copy of cdds-testing-project View status of your working copy Get latest changes Switching between branches Create a new branch Commit and Push Changes: Commit Changes Push Changes Commit Messages Template Rules Examples Simple log message example: Log message with body example: Viewing history of commits Seeing all full commit messages decreasing history Seeing first line of the commit messages decreasing history Viewing branches Seeing the information on the branch you are on: List all child branches Reset changes Reset all local changes Reset changes of a specific file Reset changes of last n commits: Reset to the last commit: Reset to the previous last commit: Revert changes Undo changes of a specific commit: Stash changes Stashing your work Re-applying your stashed changes Delete Branches Getting up to speed with git The develop branch is called main. When you start working on the project, first of all you need to get a working copy on your developer environment: Clone working copy General git clone For the cdds-inline-testing project, we use ssh. You can import the ssh key into seahorse. That prevents you to type in your git credentials each time when you use a git command. Therefore, you need to use the SSH repository URL: git clone git@github.com: / .git Clone the CDDS project For the CDDS project, we use ssh. You can import the ssh key into seahorse. That prevents you to type in your git credentials each time when you use a git command. Using SSH Key Generate a SSH key for Github or use an already existing key. You can do it via seahorse or on the command line. Generate SSH key for Github using Seahorse: Start seahorse Click on File in the menu and choose New A creation window should pop up. In that, choose Secure Shell Key. Then click on continue Now, you need to add a description of your key, for example \u201cGithub\u201d. Click on Just Create Key It will ask you for a password. You can also use an empty password but it is highly recommended to use one. Click at OK. You need to confirm your chosen password again. After confirmation, the key will be created. You should see the key in the Open SSH keys tab. Now, open a terminal. You can find the public key that you need for Github in your home folder: cd ~/.ssh Your new created public key is id_rsa. .pub file, where is the highest number of the id_rsa files. To get the key phrase that is needed for Github, simply open the public key file with an editor of your choice, for example with vi: vi id_rsa. .pub The file content should start with ssh-rsa. Add your SSH key to Github Login to Github and go to your account settings Go to the SSH and GPG keys tab Click on the green Add SSH key button Copy your public key phrase of your SSH key into the text box Click Add SSH key A confirm password box should be shown. Enter your Github password (not the SSH key password!) and then the key is added. You will get a email confirming that a new key has been added to your Github account. Clone working copy of cdds-testing-project Use following command to clone the cdds-testing-project: git clone git@github.com:MetOffice/CDDS.git View status of your working copy git status This command shows you the status of your working copy: On which branch you are What files are not committed but modified, added or removed If files have any conflicts during a merge, switch, etc. Get latest changes You need to pull the latest changes from upstream by hand. Therefor, use following command: git pull This gets the latest changes from the remote repository for the branch you are currently in. Switching between branches git checkout Make sure that you stash or commit any uncommited local changes first otherwise it could be that you get some awful conflicts. Create a new branch Each ticket should be developed in a new \u201cfeature\u201c branch. Also, bug releases etc. should have their own branches. Switch to the branch you want to branch of. This branch is called parent branch: git checkout Then, pull the changes from upstream. Your parent branch needs to be up to date: git pull Create the branch on your local machine and switch in this branch: git checkout -b Push the branch on github: git push origin You can check if your branch is created remotely: git branch -a This command should list your new created branch. Commit and Push Changes: After you did some changes you first need to commit them to your local repository afterwards you must push them to the remote repository. Commit Changes If you want to commit your changes locally, run: git commit It opens a vim or gvim, where you can specify your commit message. For the requirements of the commit messages see \u201cCommit Messages\u201c section. If you want to have one commit for each ticket then use for the first commit git commit and for all following commits git comment --amend. Push Changes Before pushing any changes you first need to commit them locally! If you want to push your changes, you must tell git on which branch to push them: git push To be absolutely sure, that you push on the right branch, you can add the branch name to the git command: git push origin Commit Messages Template : Rules Git does not wrap commit messages automatically. That makes it hard to see the whole message in a terminal. So following rules will apply for commit messages: The first line of the commit starts with the ticket number, after that there will be a colon and followed by a short description what the ticket is for: The message should not be longer than 72 characters! Do not wrap this first line If you need more descriptions: Leave one line empty after the first main message Write more descriptions. For better reading try not to write more than 72 characters per line Here, you can wrap the lines Try to be short and precise Examples Simple log message example: 178: Use critical log level instead error in cdds_store Log message with body example: 178: Use critical log leve instead error in cdds_store * Change general log level to critical * Print stack trace for critial logging Viewing history of commits Seeing all full commit messages decreasing history git log Seeing first line of the commit messages decreasing history git log --oneline Viewing branches Seeing the information on the branch you are on: git branch List all child branches git branch --list Reset changes Reset all local changes git reset --hard This discard all local changes to all files permanently. Reset changes of a specific file git reset --hard HEAD HEAD is your current branch. Reset changes of last n commits: git reset --hard HEAD~ The HEAD is your current branch. Reset to the last commit: git reset --hard HEAD~1 Reset to the previous last commit: git reset --hard HEAD~2 Revert changes Undo changes of a specific commit: Find the commit number: git log --online The commit number is the number before each commit message Revert changes to the commit number: git revert The git revert will undo the given commit but will create a new commit without deleting the older one. The git revert command will not touch the commits in your history, even the reverted ones. Stash changes You can temporarily stashes changes you have made to your working copy. You can work on something else and come back late and re-apply them. It helps you when you need to quickly switch context and work on something else on your local working copy. You can switches between branches your stashed changes will be still available. Stashing your work git stash That stashes your local (uncommited) changes, save them for later use and revert them from your working copy. Re-applying your stashed changes git stash pop This removes the changes from your stash and re-applies them to your working copy. If you do not want that you changes will be removed from the stash use: git stash apply The stash apply command is really helpful if you need to apply the changes on multiple branches. Delete Branches This should be only a well-considered option. You can delete a branch locally or remotely: Delete the branch locally: git branch -D This deletion command force to delete the branch even if it is un-merged. Delete the branch on github: git push origin --delete","title":"Working with Github"},{"location":"developer_documentation/mip_convert_functional_tests/","text":"","title":"Mip Convert Functional Tests"},{"location":"developer_documentation/nightly_tests/","text":"Warning This documentation is currently under construction and may not be up to date.","title":"Nightly Tests"},{"location":"developer_documentation/release_procedure/","text":"Warning This documentation is currently under construction and may not be up to date.","title":"Release Procedure"},{"location":"developer_documentation/review_guidelines/","text":"Warning This documentation is currently under construction and may not be up to date. Process The reviewer and reviewee discuss the type of review required. If time is a factor, focus effort on the High Priority items. Agree whether the reviewer can make corrections related to the Low Priority items, e.g., typos or unused imports, directly to the branch. If the reviewer feels they need more information after an initial look at the code, they request a pair review with the reviewee. The reviewer presents the review to the reviewee (with reasons for priority as necessary), either via the Jira ticket (e.g., if the feedback is minimal), a Confluence page, e-mails, or in person, as appropriate. The reviewee assesses the reviewer's recommendations and makes any appropriate changes to the code. Open a new Jira ticket for any low risk issues that do not need to be resolved immediately. The reviewee documents the important points from the review about why things were done (or not done) for whatever reasons on the Jira ticket (for future reference). The reviewee updates the CDDS Coding Guidelines appropriately. Checklist High Priority Is the code in version control? Does the code work as expected? Does the code manage the risk around availability of resources such as files, databases, mass (assess the risk though first) Does the code check for common errors? Does the code use exceptions appropriately? Are there corresponding unit tests for the code? Can the unit tests be executed? Is there corresponding documentation for the code? Can the documentation be built? Is the documentation easy to understand? Medium Priority Is the code easy to read and understand? Has repetitive code been avoided? Is the code easy to maintain? Low Priority Does the code comply to the coding standards?","title":"Review Guidelines"},{"location":"developer_documentation/unittests/","text":"Warning This documentation is currently under construction and may not be up to date. Unit Testing and pytest","title":"Unit Testing and pytest"},{"location":"developer_documentation/unittests/#unit-testing-and-pytest","text":"","title":"Unit Testing and pytest"},{"location":"operational_procedure/cmip6/","text":"Generating CMORised data with CDDS for CMIP6 simulations using the CDDS Suite See also guidance for adhoc generation of CMORised data . The definitions of the italicised phrases can be found in the Glossary . Tip Use <script> -h or <script> --help to print information about the script, including available parameters. Example A simulation for the pre-industrial control from UKESM will be used as an example in these instructions. When you see the :material-ticket: icon this means Prerequisites Before running the CDDS Operational Procedure, please ensure that: you own a CDDS operational simulation ticket (see the list of CDDS operational simulation tickets ) that will monitor the processing of a CMIP6 simulation using CDDS. you belong to the cdds group type groups on the command line to print the groups a user is in you have write permissions to moose:/adhoc/projects/cdds/ on MASS, i.e. is your moose username included in the access control list output by the command moo getacl moose:/adhoc/projects/cdds you use a bash shell. CDDS uses Conda. Conda can experience problems when running in any other shells except of bash. You can check which shell you use by following command: echo $SHELL If the result is not /bin/bash , you can switch to a bash shell by running: /bin/bash If any of the above are not true please contact the CDDS Team for guidance. Note Previously a .cdds_credentials file was required in order to connect to CREM, but this is no longer required for CDDS users following the retirement of CREM. Partial processing of a simulation In certain circumstances it may be desirable to process and submit a subset of an entire simulation, i.e. the first 250 years of the esm-piControl simulation. Please contact the CDDS Team to discuss this prior to starting processing to Get appropriate guidance on the steps needed to correctly construct the requested variables file in CDDS Prepare Arrange for an appropriate Errata to be issued following submission of data sets. What to do when things go wrong On occasion issues will arise with tasks performed by users of CDDS and these will trigger CRITICAL error messages in the logs and usually require user intervention. Many simple issues (MASS/MOOSE or file system problems) can be resolved by re-triggering tasks. When you take any action please ensure that you update your CDDS operational simulation ticket and if support is needed contact the CDDS Team . Set up the CDDS operational simulation ticket :material-ticket: Select start work on the CDDS operational simulation ticket (so that the status is in_progress ) to indicate that work is starting. Activate the CDDS install MOHC Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 2.5.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . JASMIN Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 2.5.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 2.5.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . Notes: * The available version numbers for this script can be found here , but the above command may lead to unexpected results if you attempt activate versions before 1.1.2 . * If you wish to deactivate the CDDS environment then you can use the command conda deactivate . Create the request JSON file Following the retirement of CREM the request JSON file is constructed from information in the rose-suite.info files within each suite. Important the rose-suite.info file contains incorrect information this will be propagated through CDDS. As such it is critically important that the information in these files is correct To construct the request JSON file take the following steps Set up a working directory mkdir cdds-example-1 cd cdds-example-1 export WORKING_DIR = ` pwd ` Add the location of your working directory to the CDDS operational simulation ticket . Collect required information on the rose suite for the simulation; suite id, e.g. u-aw310 branch, e.g. cdds revision; If in doubt use the following command to find the latest revision of the suite branch rosie lookup --prefix = u --query project eq u-cmip6 and idx eq <suite id> and branch eq <branch> Create the request JSON file; write_rose_suite_request_json <suite id> <branch> <revision> <package name> [ <list of streams> ] e.g. write_rose_suite_request_json u-aw310 cdds 115492 round-20 ap4 ap5 ap6 onm inm If necessary the start and end dates for processing can be overridden using the --start_date and --end_date arguments. Please consult with the CDDS Team if you believe this is necessary. For information: the log file and request JSON file are written to the current working directory Prepare a list of variables to process Warning This method does not refer to the data request or CDDS inventory database (to check which datasets have been previously produced), so care should be taken with the choice of variables. Create a text file with the list of variables or copy and modify an existing list. Each line in the file should have the form <mip table>/<variable name>:<stream> e.g. Amon/tas:ap5 If you are using a suite with the CMIP6 STASH set up then you can add the default stream to a list of variables using the command stream_mappings --varfile <filename without streams> --outfile <new file with streams> If you are not using a suite with the CMIP6 STASH configuration then contact us for advice as this process will need to be performed by hand. Checkout and configure the CDDS suite Run the following command after replacing values within <> checkout_processing_workflow <name for processing suite> \\ <path to request JSON> \\ <path to text file with list of variables> \\ --workflow_destination . e.g. checkout_processing_workflow my-cdds-test \\ ~hadmm/CDDS/example/request.json \\ ~hadmm/CDDS/example/variables \\ --workflow_destination . A directory containing a rose workflow will be placed in a subdirectory under the location specified in --suite_destination. If this is not specified it will be checked out under ~/roses/ Launch the suite cd into the new directory (\"HadGEM3-LL_historical_r1_round-64\") and run rose edit All useful fields are under the \"suite conf\" -> \"General Configuration\" or \"Convert Configuration\" sections. You can remove streams at this point if you do not wish to include them in the processing. For CMIP6 production work the output mass root should be left blank and the output mass suffix should be production . The output location should be set to \"project\", i.e. /project/cdds_data/ and /project/cdds/proc/ Monitor conversion suites Each of the suites launched by CDDS Convert requires monitoring. This can be done using the command line tool cylc gui to obtain a window with an updating summary of suites progress or equivalently the Cylc Review online tools. Conversion suites will usually be named cdds_<model id>_<experiment id>_<variant_label>_<stream> and each stream will run completely independently. If a suite has issues, due to task failure, it will stall and you will receive an e-mail. If you hit issues or are unsure how to proceed update the CDDS operational simulation ticket for your package with anything you believe is relevant (include the location of your working directory) and contact the CDDS Team for advice. The conversion suites run the following steps run_extract_<stream> run_extract_<\\stream> Run CDDS Extract for this stream. Runs in long queue with a wall time of 2 days. If there are any issues with extracting data they will be reported in the job.err log file in the suite and the $CDDS_PROC_DIR/extract/log/cdds_extract_<stream>_<date stamp>.log log file and the task will fail. The extraction task will automatically resubmit 4 times if it fails and manual intervention is required to proceed. Most issues are related to either MASS (i.e. moo commands failing), file system anomalies (failure to create files /directories) or running out of time. Identify issues either by searching for \"CRITICAL\" in the job.err logs in cylc review or by using grep CRITICAL $CDDS_PROC_DIR /extract/log/cdds_extract_<stream>_<date stamp>.log If the issue appears to be due to MASS issues you can re-run the failed CDDS Extract job by re-triggering the run_extract_<stream> task via the cylc gui or via the cylc command line tools; cylc trigger cdds_<model id>_<experiment id>_<variant label>_<stream> run_extract_<stream>:failed If in doubt update your CDDS operational simulation ticket and contact CDDS Team for advice. validate_extract_<stream> Validation of the output is now performed as a separate task from extracting it. This task will report missing or unexpected files and unreadable netcdf files. setup_output_dir_<stream> Create output directories for conversion output mip_convert_<stream>_<grid group> Run MIP Convert to produce output files for a small time window for this simulation. Will retry up to 3 times before suite stalls. CRITICAL issues are appended to $CDDS_PROC_DIR/convert/log/critical_issues.log (this file will not exist if no critical issues arise). These will likely need user action to correct for -- update your CDDS operational simulation ticket and contact CDDS Team for advice. A variant named mip_convert_first_<stream>_<grid group> may be launched to align the cycling dates with the concatenation processing. finaliser_<stream> Ensures that concatenation tasks are launched once all MIP Convert tasks have been successfully performed for a particular time range. Should never fail organise_files_<stream> Re-arranges the output files on disk from a directory structure created by the MIP Convert tasks of the form bash $CDDS_DATA_DIR/output/<stream>_mip_convert/<YYYY-MM-DD>/<grid>/<files> to bash $CDDS_DATA_DIR/output/<stream>_concat/<MIP table>/<variable name>/<files> ready for concatenation. A variation named organise_files_final_<stream> does the same thing but at the end of the conversion process. mip_concatenate_setup_<stream> construct a list of concatenation jobs that must be performed mip_concatenate_batch_<stream> Perform the concatenation commands ( ncrcat ) required to join small files together. Runs in long queue with a wall time of 2 days and can retry up to 3 times before suite stalls (failures are usually due to running out of time while performing a concatenation). Only one mip_concatenate_batch_<stream> task can run at one time. Issues can be identified using grep CRITICAL $CDDS_PROC_DIR /convert/log/mip_concatenate_*.log if any critical issues arise or tasks fail update your CDDS operational simulation ticket and contact the CDDS Team for advice. Output data is written to $CDDS_DATA_DIR /output/<stream>/<MIP table>/<variable name>/<files> run_qc_<stream> Run the QC process on output data for this stream Produces a report at $CDDS_PROC_DIR /qualitycheck/report_<stream>_<datestamp>.json and a list of variables which pass the quality checks at $CDDS_PROC_DIR /qualitycheck/approved_variables_<stream>_<datestamp>.txt and a log file at $CDDS_PROC_DIR /qualitycheck/log/qc_run_and_report_<stream>_<datestamp>.log An example of a clean QC report can be found at wiki:Quality/ExampleJSONReport and the approved variables file will have one line per successfully produced dataset of the form <MIP table>/<variable name>;<Directory containing files> This task will fail if any QC issues are found and will not resubmit. If this occurs please update your CDDS operational simulation ticket and contact the CDDS Team for advice. run_transfer_<stream> Archive data for variables that are marked active in the requested variables file produced by CDDS Prepare and have successfully passed the QC checks, i.e. are listed in the approved variables file. Will not automatically retry, even if failure was due to MASS/MOOSE issues. the location in MASS to which these data are archived is determined by the --output_mass_suffix argument specified in the cdds_convert command above. Task will fail if There are MASS issues: For example if the following command returns anything there has been a MASS outage and you can re-trigger the task: grep SSC_STORAGE_SYSTEM_UNAVAILABLE $CDDS_PROC_DIR /archive/log/cdds_store_<stream>_<date stamp>.log An attempt is made to archive data that already exists in MASS. If this occurs please update your CDDS operational simulation ticket and contact the CDDS Team for advice. VERY IMPORTANT: do not delete data from MASS without consultation with Matt Mizielinski . completion_<stream> This is a dummy task that is the last thing to run in the suite -- this is to allow inter suite dependencies by allowing the CDDS workflow to monitor whether each per stream workflow has completed. If all goes well the suite will complete and you will receive an email confirming that the suite has shutdown containing content of the form Message: AUTOMATIC See: http://fcm1/cylc-review/taskjobs/<user id>/<suite name> Prepare CDDS operational simulation ticket for review & submission Once all suites for a particular package have completed (use cylc gscan or cylc review to monitor this) update your CDDS operational simulation ticket confirming that the Extract, Convert, QC and Transfer tasks have been completed. Copy the request JSON file and any logs to $CDDS_PROC_DIR cp request.json *.log $CDDS_PROC_DIR / Ticket : Add a comment to the CDDS operational simulation ticket specifying the archived data is ready for submission, and include the full path of the $CDDS_PROC_DIR and $CDDS_DATA_DIR locations. Select assign for review to on the CDDS operational simulation ticket (so that the status is reviewing ) and assign the CDDS operational simulation ticket to Matthew Mizielinski by selecting this name from the list The ticket will then be reviewed according to the [wiki:SimulationTicketReview CDDS review procedure] by members of the CDDS team. The review script used by the CDDS team involves running the following command cdds_sim_review $CDDS_PROC_DIR $CDDS_DATA_DIR checking any CRITICAL issues and following up any other anomalies. Run CDDS Teardown Once the approved ticket has been returned to you following submission, delete the contents of the data directory: cd $CDDS_DATA_DIR rm -rf input output Delete all suites used: rose suite-clean cdds_<model_id>_<experiment_id>_<variant_label>_<stream> for each <stream> processed, or ls -d ~/cylc-run/cdds_<model_id>_<experiment_id>_<variant_label>_* | xargs rose suite-clean -y which should find and clear all suites associated with the model, experiment and variant label specified. Ticket : Update and close the CDDS operational simulation ticket","title":"CMIP6"},{"location":"operational_procedure/cmip6/#generating-cmorised-data-with-cdds-for-cmip6-simulations-using-the-cdds-suite","text":"See also guidance for adhoc generation of CMORised data . The definitions of the italicised phrases can be found in the Glossary . Tip Use <script> -h or <script> --help to print information about the script, including available parameters. Example A simulation for the pre-industrial control from UKESM will be used as an example in these instructions. When you see the :material-ticket: icon this means","title":"Generating CMORised data with CDDS for CMIP6 simulations using the CDDS Suite"},{"location":"operational_procedure/cmip6/#prerequisites","text":"Before running the CDDS Operational Procedure, please ensure that: you own a CDDS operational simulation ticket (see the list of CDDS operational simulation tickets ) that will monitor the processing of a CMIP6 simulation using CDDS. you belong to the cdds group type groups on the command line to print the groups a user is in you have write permissions to moose:/adhoc/projects/cdds/ on MASS, i.e. is your moose username included in the access control list output by the command moo getacl moose:/adhoc/projects/cdds you use a bash shell. CDDS uses Conda. Conda can experience problems when running in any other shells except of bash. You can check which shell you use by following command: echo $SHELL If the result is not /bin/bash , you can switch to a bash shell by running: /bin/bash If any of the above are not true please contact the CDDS Team for guidance. Note Previously a .cdds_credentials file was required in order to connect to CREM, but this is no longer required for CDDS users following the retirement of CREM.","title":"Prerequisites"},{"location":"operational_procedure/cmip6/#partial-processing-of-a-simulation","text":"In certain circumstances it may be desirable to process and submit a subset of an entire simulation, i.e. the first 250 years of the esm-piControl simulation. Please contact the CDDS Team to discuss this prior to starting processing to Get appropriate guidance on the steps needed to correctly construct the requested variables file in CDDS Prepare Arrange for an appropriate Errata to be issued following submission of data sets.","title":"Partial processing of a simulation"},{"location":"operational_procedure/cmip6/#what-to-do-when-things-go-wrong","text":"On occasion issues will arise with tasks performed by users of CDDS and these will trigger CRITICAL error messages in the logs and usually require user intervention. Many simple issues (MASS/MOOSE or file system problems) can be resolved by re-triggering tasks. When you take any action please ensure that you update your CDDS operational simulation ticket and if support is needed contact the CDDS Team .","title":"What to do when things go wrong"},{"location":"operational_procedure/cmip6/#set-up-the-cdds-operational-simulation-ticket","text":":material-ticket: Select start work on the CDDS operational simulation ticket (so that the status is in_progress ) to indicate that work is starting.","title":"Set up the CDDS operational simulation ticket"},{"location":"operational_procedure/cmip6/#activate-the-cdds-install","text":"MOHC Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 2.5.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . JASMIN Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 2.5.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . Setup the environment to use the central installation of CDDS and its dependencies: source ~cdds/bin/setup_env_for_cdds <cdds_version> where <cdds_version> is the version of CDDS you wish to use, e.g. 2.5.0 . Unless instructed otherwise you should use the most recent version of CDDS available (to ensure that all bugfixes are picked up), and this version should be used in all stages of the package being processed. If in doubt contact the CDDS team for advice. Ticket : Record the version of CDDS being used on the CDDS operational simulation ticket . Notes: * The available version numbers for this script can be found here , but the above command may lead to unexpected results if you attempt activate versions before 1.1.2 . * If you wish to deactivate the CDDS environment then you can use the command conda deactivate .","title":"Activate the CDDS install"},{"location":"operational_procedure/cmip6/#create-the-request-json-file","text":"Following the retirement of CREM the request JSON file is constructed from information in the rose-suite.info files within each suite. Important the rose-suite.info file contains incorrect information this will be propagated through CDDS. As such it is critically important that the information in these files is correct To construct the request JSON file take the following steps Set up a working directory mkdir cdds-example-1 cd cdds-example-1 export WORKING_DIR = ` pwd ` Add the location of your working directory to the CDDS operational simulation ticket . Collect required information on the rose suite for the simulation; suite id, e.g. u-aw310 branch, e.g. cdds revision; If in doubt use the following command to find the latest revision of the suite branch rosie lookup --prefix = u --query project eq u-cmip6 and idx eq <suite id> and branch eq <branch> Create the request JSON file; write_rose_suite_request_json <suite id> <branch> <revision> <package name> [ <list of streams> ] e.g. write_rose_suite_request_json u-aw310 cdds 115492 round-20 ap4 ap5 ap6 onm inm If necessary the start and end dates for processing can be overridden using the --start_date and --end_date arguments. Please consult with the CDDS Team if you believe this is necessary. For information: the log file and request JSON file are written to the current working directory","title":"Create the request JSON file"},{"location":"operational_procedure/cmip6/#prepare-a-list-of-variables-to-process","text":"Warning This method does not refer to the data request or CDDS inventory database (to check which datasets have been previously produced), so care should be taken with the choice of variables. Create a text file with the list of variables or copy and modify an existing list. Each line in the file should have the form <mip table>/<variable name>:<stream> e.g. Amon/tas:ap5 If you are using a suite with the CMIP6 STASH set up then you can add the default stream to a list of variables using the command stream_mappings --varfile <filename without streams> --outfile <new file with streams> If you are not using a suite with the CMIP6 STASH configuration then contact us for advice as this process will need to be performed by hand.","title":"Prepare a list of variables to process"},{"location":"operational_procedure/cmip6/#checkout-and-configure-the-cdds-suite","text":"Run the following command after replacing values within <> checkout_processing_workflow <name for processing suite> \\ <path to request JSON> \\ <path to text file with list of variables> \\ --workflow_destination . e.g. checkout_processing_workflow my-cdds-test \\ ~hadmm/CDDS/example/request.json \\ ~hadmm/CDDS/example/variables \\ --workflow_destination . A directory containing a rose workflow will be placed in a subdirectory under the location specified in --suite_destination. If this is not specified it will be checked out under ~/roses/","title":"Checkout and configure the CDDS suite"},{"location":"operational_procedure/cmip6/#launch-the-suite","text":"cd into the new directory (\"HadGEM3-LL_historical_r1_round-64\") and run rose edit All useful fields are under the \"suite conf\" -> \"General Configuration\" or \"Convert Configuration\" sections. You can remove streams at this point if you do not wish to include them in the processing. For CMIP6 production work the output mass root should be left blank and the output mass suffix should be production . The output location should be set to \"project\", i.e. /project/cdds_data/ and /project/cdds/proc/","title":"Launch the suite"},{"location":"operational_procedure/cmip6/#monitor-conversion-suites","text":"Each of the suites launched by CDDS Convert requires monitoring. This can be done using the command line tool cylc gui to obtain a window with an updating summary of suites progress or equivalently the Cylc Review online tools. Conversion suites will usually be named cdds_<model id>_<experiment id>_<variant_label>_<stream> and each stream will run completely independently. If a suite has issues, due to task failure, it will stall and you will receive an e-mail. If you hit issues or are unsure how to proceed update the CDDS operational simulation ticket for your package with anything you believe is relevant (include the location of your working directory) and contact the CDDS Team for advice. The conversion suites run the following steps run_extract_<stream> run_extract_<\\stream> Run CDDS Extract for this stream. Runs in long queue with a wall time of 2 days. If there are any issues with extracting data they will be reported in the job.err log file in the suite and the $CDDS_PROC_DIR/extract/log/cdds_extract_<stream>_<date stamp>.log log file and the task will fail. The extraction task will automatically resubmit 4 times if it fails and manual intervention is required to proceed. Most issues are related to either MASS (i.e. moo commands failing), file system anomalies (failure to create files /directories) or running out of time. Identify issues either by searching for \"CRITICAL\" in the job.err logs in cylc review or by using grep CRITICAL $CDDS_PROC_DIR /extract/log/cdds_extract_<stream>_<date stamp>.log If the issue appears to be due to MASS issues you can re-run the failed CDDS Extract job by re-triggering the run_extract_<stream> task via the cylc gui or via the cylc command line tools; cylc trigger cdds_<model id>_<experiment id>_<variant label>_<stream> run_extract_<stream>:failed If in doubt update your CDDS operational simulation ticket and contact CDDS Team for advice. validate_extract_<stream> Validation of the output is now performed as a separate task from extracting it. This task will report missing or unexpected files and unreadable netcdf files. setup_output_dir_<stream> Create output directories for conversion output mip_convert_<stream>_<grid group> Run MIP Convert to produce output files for a small time window for this simulation. Will retry up to 3 times before suite stalls. CRITICAL issues are appended to $CDDS_PROC_DIR/convert/log/critical_issues.log (this file will not exist if no critical issues arise). These will likely need user action to correct for -- update your CDDS operational simulation ticket and contact CDDS Team for advice. A variant named mip_convert_first_<stream>_<grid group> may be launched to align the cycling dates with the concatenation processing. finaliser_<stream> Ensures that concatenation tasks are launched once all MIP Convert tasks have been successfully performed for a particular time range. Should never fail organise_files_<stream> Re-arranges the output files on disk from a directory structure created by the MIP Convert tasks of the form bash $CDDS_DATA_DIR/output/<stream>_mip_convert/<YYYY-MM-DD>/<grid>/<files> to bash $CDDS_DATA_DIR/output/<stream>_concat/<MIP table>/<variable name>/<files> ready for concatenation. A variation named organise_files_final_<stream> does the same thing but at the end of the conversion process. mip_concatenate_setup_<stream> construct a list of concatenation jobs that must be performed mip_concatenate_batch_<stream> Perform the concatenation commands ( ncrcat ) required to join small files together. Runs in long queue with a wall time of 2 days and can retry up to 3 times before suite stalls (failures are usually due to running out of time while performing a concatenation). Only one mip_concatenate_batch_<stream> task can run at one time. Issues can be identified using grep CRITICAL $CDDS_PROC_DIR /convert/log/mip_concatenate_*.log if any critical issues arise or tasks fail update your CDDS operational simulation ticket and contact the CDDS Team for advice. Output data is written to $CDDS_DATA_DIR /output/<stream>/<MIP table>/<variable name>/<files> run_qc_<stream> Run the QC process on output data for this stream Produces a report at $CDDS_PROC_DIR /qualitycheck/report_<stream>_<datestamp>.json and a list of variables which pass the quality checks at $CDDS_PROC_DIR /qualitycheck/approved_variables_<stream>_<datestamp>.txt and a log file at $CDDS_PROC_DIR /qualitycheck/log/qc_run_and_report_<stream>_<datestamp>.log An example of a clean QC report can be found at wiki:Quality/ExampleJSONReport and the approved variables file will have one line per successfully produced dataset of the form <MIP table>/<variable name>;<Directory containing files> This task will fail if any QC issues are found and will not resubmit. If this occurs please update your CDDS operational simulation ticket and contact the CDDS Team for advice. run_transfer_<stream> Archive data for variables that are marked active in the requested variables file produced by CDDS Prepare and have successfully passed the QC checks, i.e. are listed in the approved variables file. Will not automatically retry, even if failure was due to MASS/MOOSE issues. the location in MASS to which these data are archived is determined by the --output_mass_suffix argument specified in the cdds_convert command above. Task will fail if There are MASS issues: For example if the following command returns anything there has been a MASS outage and you can re-trigger the task: grep SSC_STORAGE_SYSTEM_UNAVAILABLE $CDDS_PROC_DIR /archive/log/cdds_store_<stream>_<date stamp>.log An attempt is made to archive data that already exists in MASS. If this occurs please update your CDDS operational simulation ticket and contact the CDDS Team for advice. VERY IMPORTANT: do not delete data from MASS without consultation with Matt Mizielinski . completion_<stream> This is a dummy task that is the last thing to run in the suite -- this is to allow inter suite dependencies by allowing the CDDS workflow to monitor whether each per stream workflow has completed. If all goes well the suite will complete and you will receive an email confirming that the suite has shutdown containing content of the form Message: AUTOMATIC See: http://fcm1/cylc-review/taskjobs/<user id>/<suite name>","title":"Monitor conversion suites"},{"location":"operational_procedure/cmip6/#prepare-cdds-operational-simulation-ticket-for-review-submission","text":"Once all suites for a particular package have completed (use cylc gscan or cylc review to monitor this) update your CDDS operational simulation ticket confirming that the Extract, Convert, QC and Transfer tasks have been completed. Copy the request JSON file and any logs to $CDDS_PROC_DIR cp request.json *.log $CDDS_PROC_DIR / Ticket : Add a comment to the CDDS operational simulation ticket specifying the archived data is ready for submission, and include the full path of the $CDDS_PROC_DIR and $CDDS_DATA_DIR locations. Select assign for review to on the CDDS operational simulation ticket (so that the status is reviewing ) and assign the CDDS operational simulation ticket to Matthew Mizielinski by selecting this name from the list The ticket will then be reviewed according to the [wiki:SimulationTicketReview CDDS review procedure] by members of the CDDS team. The review script used by the CDDS team involves running the following command cdds_sim_review $CDDS_PROC_DIR $CDDS_DATA_DIR checking any CRITICAL issues and following up any other anomalies.","title":"Prepare CDDS operational simulation ticket for review &amp; submission"},{"location":"operational_procedure/cmip6/#run-cdds-teardown","text":"Once the approved ticket has been returned to you following submission, delete the contents of the data directory: cd $CDDS_DATA_DIR rm -rf input output Delete all suites used: rose suite-clean cdds_<model_id>_<experiment_id>_<variant_label>_<stream> for each <stream> processed, or ls -d ~/cylc-run/cdds_<model_id>_<experiment_id>_<variant_label>_* | xargs rose suite-clean -y which should find and clear all suites associated with the model, experiment and variant label specified. Ticket : Update and close the CDDS operational simulation ticket","title":"Run CDDS Teardown"},{"location":"operational_procedure/cmip6_plus/","text":"","title":"CMIP6 Plus"},{"location":"operational_procedure/gcmodeldev/","text":"","title":"GCModelDev"},{"location":"reference/SUMMARY/","text":"","title":"API Documentation"},{"location":"tutorials/add_mapping/","text":"how to add a mapping","title":"Add a Variable Mapping"},{"location":"tutorials/add_plugin/","text":"","title":"Add a Plugin"},{"location":"tutorials/search_inventory/","text":"Script: search_inventory In release v1.6.0 we've added an inventory (updated each night) that is used by CDDS Prepare to avoid the need for combined \"approved_variables\" files to avoid duplicating previously provided data. The script allows the user to search the inventory using data set id patterns which can include wildcards instead of a particular \"facet\". The inventory database We generate a small sqlite3 database each night from the contents of MASS. As such this only contains data that we have produced within the Met Office and archived to MASS. Data set ids and \"facet\" structure The dataset ids used in CMIP6 are made up of a set of \"facets\", strings describing some aspect of what the data relates to. A facet may be the name of the MIP, experiment_id or variable name. The format for CMIP6 is CMIP6.<activity id (MIP)>.<institution id>.<source_id (model)>.<experiment id>.<variant label>.<MIP table>.<variable id>.<grid label> e.g. CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.historical.r1i1p1f2.Amon.tas.gn Search for datasets matching a facet pattern. The data set id used for the inventory is a set of 9 facets joined by dots. To search for all UKESM1 daily precipitation datasets you can run search_inventory CMIP6.*.*.UKESM1-0-LL.*.*.day.pr.* Which at the time of writing returns Mip Era Mip Institute Model Experiment Variant Mip Table Variable Name Grid Status Version Facet String CMIP6 AerChemMIP MOHC UKESM1-0-LL hist-piAer r1i1p1f2 day pr gn available v20190813 CMIP6.AerChemMIP.MOHC.UKESM1-0-LL.hist-piAer.r1i1p1f2.day.pr.gn CMIP6 AerChemMIP MOHC UKESM1-0-LL hist-piAer r2i1p1f2 day pr gn available v20191104 CMIP6.AerChemMIP.MOHC.UKESM1-0-LL.hist-piAer.r2i1p1f2.day.pr.gn ... CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r4i1p1f2 day pr gn available v20190814 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r4i1p1f2.day.pr.gn CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r8i1p1f2 day pr gn available v20190906 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r8i1p1f2.day.pr.gn A total of 239 records were found. Locating data in MASS If you require the list of files in MASS for a particular dataset then the -s option will provide this for each data set matching the pattern in the inventory. For example search_inventory CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.*.pr.gn -s returns Mip Era Mip Institute Model Experiment Variant Mip Table Variable Name Grid Status Version Facet String CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r3i1p1f2 Amon pr gn available v20190507 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.Amon.pr.gn moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507 moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507/pr_Amon_UKESM1-0-LL_ssp585_r3i1p1f2_gn_201501-204912.nc moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507/pr_Amon_UKESM1-0-LL_ssp585_r3i1p1f2_gn_205001-210012.nc CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r3i1p1f2 day pr gn available v20190813 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.day.pr.gn moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813 moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813/pr_day_UKESM1-0-LL_ssp585_r3i1p1f2_gn_20150101-20491230.nc moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813/pr_day_UKESM1-0-LL_ssp585_r3i1p1f2_gn_20500101-21001230.nc A total of 2 records were found. Note that in this case a moo ls command is run for each data set found.","title":"Search Inventory"},{"location":"tutorials/search_inventory/#script-search_inventory","text":"In release v1.6.0 we've added an inventory (updated each night) that is used by CDDS Prepare to avoid the need for combined \"approved_variables\" files to avoid duplicating previously provided data. The script allows the user to search the inventory using data set id patterns which can include wildcards instead of a particular \"facet\".","title":"Script: search_inventory"},{"location":"tutorials/search_inventory/#the-inventory-database","text":"We generate a small sqlite3 database each night from the contents of MASS. As such this only contains data that we have produced within the Met Office and archived to MASS.","title":"The inventory database"},{"location":"tutorials/search_inventory/#data-set-ids-and-facet-structure","text":"The dataset ids used in CMIP6 are made up of a set of \"facets\", strings describing some aspect of what the data relates to. A facet may be the name of the MIP, experiment_id or variable name. The format for CMIP6 is CMIP6.<activity id (MIP)>.<institution id>.<source_id (model)>.<experiment id>.<variant label>.<MIP table>.<variable id>.<grid label> e.g. CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.historical.r1i1p1f2.Amon.tas.gn","title":"Data set ids and \"facet\" structure"},{"location":"tutorials/search_inventory/#search-for-datasets-matching-a-facet-pattern","text":"The data set id used for the inventory is a set of 9 facets joined by dots. To search for all UKESM1 daily precipitation datasets you can run search_inventory CMIP6.*.*.UKESM1-0-LL.*.*.day.pr.* Which at the time of writing returns Mip Era Mip Institute Model Experiment Variant Mip Table Variable Name Grid Status Version Facet String CMIP6 AerChemMIP MOHC UKESM1-0-LL hist-piAer r1i1p1f2 day pr gn available v20190813 CMIP6.AerChemMIP.MOHC.UKESM1-0-LL.hist-piAer.r1i1p1f2.day.pr.gn CMIP6 AerChemMIP MOHC UKESM1-0-LL hist-piAer r2i1p1f2 day pr gn available v20191104 CMIP6.AerChemMIP.MOHC.UKESM1-0-LL.hist-piAer.r2i1p1f2.day.pr.gn ... CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r4i1p1f2 day pr gn available v20190814 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r4i1p1f2.day.pr.gn CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r8i1p1f2 day pr gn available v20190906 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r8i1p1f2.day.pr.gn A total of 239 records were found.","title":"Search for datasets matching a facet pattern."},{"location":"tutorials/search_inventory/#locating-data-in-mass","text":"If you require the list of files in MASS for a particular dataset then the -s option will provide this for each data set matching the pattern in the inventory. For example search_inventory CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.*.pr.gn -s returns Mip Era Mip Institute Model Experiment Variant Mip Table Variable Name Grid Status Version Facet String CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r3i1p1f2 Amon pr gn available v20190507 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.Amon.pr.gn moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507 moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507/pr_Amon_UKESM1-0-LL_ssp585_r3i1p1f2_gn_201501-204912.nc moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/Amon/pr/gn/available/v20190507/pr_Amon_UKESM1-0-LL_ssp585_r3i1p1f2_gn_205001-210012.nc CMIP6 ScenarioMIP MOHC UKESM1-0-LL ssp585 r3i1p1f2 day pr gn available v20190813 CMIP6.ScenarioMIP.MOHC.UKESM1-0-LL.ssp585.r3i1p1f2.day.pr.gn moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813 moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813/pr_day_UKESM1-0-LL_ssp585_r3i1p1f2_gn_20150101-20491230.nc moose:/adhoc/projects/cdds/production/CMIP6/ScenarioMIP/MOHC/UKESM1-0-LL/ssp585/r3i1p1f2/day/pr/gn/available/v20190813/pr_day_UKESM1-0-LL_ssp585_r3i1p1f2_gn_20500101-21001230.nc A total of 2 records were found. Note that in this case a moo ls command is run for each data set found.","title":"Locating data in MASS"}]}